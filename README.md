# Helm и Kubernetes

## Установка Helm
В этом руководстве рассказывается, как установить Helm CLI. Helm может быть установлен как из исходного кода, так и из собранных binary релизов.

## Из Проекта Helm
Проект Helm предоставляет два способа получения и установки Helm. Это официальные методы получения релизов Helm. В дополнение к этому сообщество Helm предоставляет методы установки Helm через различные менеджеры пакетов. Установка с помощью этих методов может быть найдена ниже официальных методов.

## Из Binary Релизов
Каждый релиз Helm предоставляет различные binary сборки под разные операционные системы (OSes). Эти binary версии могут быть загружены и установлены вручную.
1.	Скачайте нужную вам версию
2.	Распакуйте её (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)
3.	Найдите helm binary файл в директории из распаковки, и переместите в нужное место (mv linux-amd64/helm /usr/local/bin/helm)
Сразу после этого можно запустить Helm справку добавить stable репозиторий: helm help.
### Примечание: Автоматические тесты Helm выполняются для Linux AMD64 только во время сборки и выпуска CircleCI. Тестирование других OSes является обязанностью сообщества, использующего Helm на их OS.

## Из Скрипта
У Helm теперь есть скрипт установки, которая будет автоматически загружать последнюю версию Helm и устанавливать его локально.
Вы можете получить этот сценарий, а затем выполнить его локально. Он хорошо документирован, так что вы можете прочитать его и понять, что он делает, прежде чем запускать.

$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3

$ chmod 700 get_helm.sh

$ ./get_helm.sh

Да, можно закрутить curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash , если вы хотите острых ощущений.

## Через Менеджеров Пакетов
Сообщество Helm предоставляет возможность установки Helm через менеджеров пакетов операционной системы. Они не поддерживаются проектом Helm и не считаются проверенными.

## Используя Homebrew (macOS)
Члены сообщества Helm внесли свой вклад в создание formula Helm для Homebrew. Эта formula, почти всегда, актуальна.

brew install helm

### (Примечание: Существует также formula для emacs-helm, являющаяся другим проектом.)

## Используя Chocolatey (Windows)
Члены сообщества Helm внесли свой вклад в Helm пакет собранный под Chocolatey. Данная сборка почти всегда актуальна

choco install kubernetes-helm

## Используя Apt (Debian/Ubuntu)
Члены сообщества Helm внесли свой вклад в Helm package для Apt. Данная сборка почти всегда актуальна

curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null

sudo apt-get install apt-transport-https --yes

echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list

sudo apt-get update

sudo apt-get install helm

## Используя Snap
Snapcrafters сообщество поддерживает Snap-версию Helm пакета:

sudo snap install helm --classic

### Используя pkg (FreeBSD)
Члены сообщества FreeBSD внесли свой вклад в создание Helm пакета под FreeBSD Ports Collection. Данная сборка почти всегда актуальна

pkg install helm

## Тестовые Сборки
В дополнение к релизам вы можете скачать или установить тестовые сборки

## Используя Canary Сборки
Сборки Canary – это версии программного обеспечения Helm, построенные из последней основной ветви. Они не являются официальными релизами и могут быть нестабильными. Тем не менее, они предлагают возможность протестировать передовые функции.
Canary Helm binaries сборки хранятся в get.helm.sh. Вот несколько ссылок на общие сборки:
•	Linux AMD64
•	macOS AMD64
•	Experimental Windows AMD64

## Из Исходного Кода (Linux, macOS)
Создание Helm из исходного кода-это немного больше работы, но это лучший способ пойти, если вы хотите протестировать последнюю версию (pre-release) Helm.
У вас должна быть рабочая среда Go.

$ git clone https://github.com/helm/helm.git

$ cd helm

$ make

При необходимости скрипт извлекает зависимости, кэширует их и проверяет конфигурацию. Затем он скомпилирует helm и поместит его в bin/helm.

## Заключение
В большинстве случаев установка так же проста, как получение предварительно построенного binary файла helm. В этом документе описаны дополнительные случаи для тех, кто хочет делать с Helm более сложные вещи.
После успешной установки клиента Helm вы можете перейти к использованию Helm для управления chart-ми и добавить stable репозиторий.


# Создание кластера Kubernetes с помощью Kubeadm в Ubuntu 18.04 на платформе Базис. Digital Energy

## Введение
Kubernetes — это система оркестровки контейнеров, обеспечивающая управление контейнерами в масштабе. Система Kubernetes была первоначально разработана Google на основе опыта компании в использовании контейнеров в рабочей среде. Это решение с открытым исходным кодом, и в его разработке активно участвуют представители сообщества разработчиков со всего мира.
### Примечание. 
В этом обучающем модуле используется версия Kubernetes 1.24, последняя официальная поддерживаемая версия на момент публикации данной статьи. Актуальную информацию о последней версии можно найти в текущих примечаниях к выпуску в официальной документации Kubernetes.
Kubeadm автоматизирует установку и настройку компонентов Kubernetes, в том числе сервера API, Controller Manager и Kube DNS. Однако данное средство не создает пользователей и не выполняет установку зависимостей уровня операционной системы и их конфигурации. Для предварительных задач существует возможность использования инструментов управления конфигурацией, таких как Ansible и SaltStack. Использование этих инструментов упрощает создание дополнительных кластеров или воссоздание существующих кластеров, а также снижает вероятность ошибок.
В этом обучающем модуле вы научитесь создавать кластер Kubernetes с помощью Ansible и Kubeadm, а затем развертывать в нем приложение Nginx в контейнерах.

## Цели
Ваш кластер будет включать следующие физические ресурсы:

•	Один главный узел

Главный узел (под узлом в Kubernetes подразумевается сервер), отвечающиой за управление состоянием кластера. На нем работает система Etcd, которая хранит данные кластера среди компонентов, распределяющих рабочие задачи по рабочим узлам.

•	Два рабочих узла

Рабочие узлы — это серверы, где выполняются рабочие задачи (т. е. приложения и службы в контейнерах). Рабочий узел продолжает выполнять назначенную задачу, даже если главный узел отключается после распределения задач. Добавление рабочих узлов позволяет увеличить объем кластера.
После прохождения данного обучающего модуля вы получите кластер, готовый к запуску приложений в контейнерах, при условии, что серверы кластера имеют достаточные ресурсы процессорной мощности и оперативной памяти для выполнения этих приложений. Практически любые традиционные приложения Unix, в том числе веб-приложения, базы данных, демоны и инструменты командной строки, можно поместить в контейнеры и запускать в кластере. Сам кластер потребляет примерно 300-500 МБ оперативной памяти и 10% ресурсов процессора на каждом узле.

После настройки кластера вы развернете веб-сервер Nginx для проверки правильного выполнения рабочих задач.

## Предварительные требования

•	Пара ключей SSH на локальном компьютере под управлением Linux/macOS/BSD.

•	Три сервера с Ubuntu 18.04, развернутые на технологической платформе ПАО Ростелеком Базис. Digital Energy, имеющие не менее 2 ГБ оперативной памяти и 2 виртуальных процессоров каждый. Вы должны иметь возможность подключаться к каждому серверу через SSH как пользователь root, используя вашу пару ключей SSH.

•	Система Ansible, установленная на локальном компьютере.

•	Знакомство с плейбуками Ansible.

•	Умение запускать контейнеры из образа Docker.


## Шаг 1 — Настройка каталога рабочего пространства и файла инвентаризации Ansible
В этом разделе вы создадите на локальном компьютере каталог, который будет выступать в качестве рабочего пространства. Вы выполните локальную настройку Ansible, чтобы обеспечить возможность связи с вашими удаленными серверами и выполнения команд на этих серверах. После этого вы создадите файл hosts с данными инвентаризации, в том числе с IP-адресами ваших серверов и данными групп, к которым принадлежит каждый сервер.
Из трех ваших серверов один сервер будет главным сервером, и его IP-адрес будет отображаться как master_ip. Другие два сервера будут рабочими узлами и будут иметь IP-адреса worker_1_ip и worker_2_ip.
Создайте каталог ~/kube-cluster в домашнем каталоге локального компьютера и перейдите в него с помощью команды cd:

mkdir ~/kube-cluster

cd ~/kube-cluster

В рамках этого обучающего модуля данный каталог будет выступать в качестве рабочего пространства, и в нем будут храниться все ваши плейбуки Ansible. Также в этом каталоге вы будете запускать все локальные команды.
Создайте файл с именем ~/kube-cluster/hosts с помощью nano или своего любимого текстового редактора:

nano ~/kube-cluster/hosts

Добавьте в файл следующий текст с информацией о логической структуре вашего кластера:

[masters]

master ansible_host=master_ip ansible_user=root

[workers]

worker1 ansible_host=worker_1_ip ansible_user=root

worker2 ansible_host=worker_2_ip ansible_user=root

[all:vars]

ansible_python_interpreter=/usr/bin/python3

Возможно вы помните, что файлы инвентаризации в Ansible используются для указания данных серверов, в том числе IP-адресов, удаленных пользователей и группировок серверов как единый объем для целей выполнения команд. Файл ~/kube-cluster/hosts будет вашим файлом инвентаризации, и вы добавили в него две группы Ansible (masters и workers) для определения логической структуры вашего кластера.
В группе masters имеется запись сервера master, в которой указан IP-адрес главного узла (master_ip) и указывается, что система Ansible должна запускать удаленные команды от имени пользователя root.
В группе workers также есть две записи для серверов рабочих узлов (worker_1_ip и worker_2_ip), где пользователь ansible_user также задается как пользователь root.
В последней строке файла Ansible предписывается использовать для операций управления интерпретаторы Python 3 удаленных серверов.
Сохраните и закройте файл после добавления текста.
После настойки инвентаризации сервера с помощью групп мы переходим к установке зависимостей уровня операционной системы и создания параметров конфигурации.

## Шаг 2 — Создание пользователя без привилегий root на всех удаленных серверах
В этом разделе вы создадите пользователя без привилегий root с привилегиями sudo на всех серверах, чтобы вы могли вручную подключаться к ним через SSH как пользователь без привилегий. Это полезно на случай, если вы захотите посмотреть информацию о системе с помощью таких команд как top/htop, просмотреть список работающих контейнеров или изменить файлы конфигурации, принадлежащие пользователю root. Данные операции обычно выполняются во время технического обслуживания кластера, и использование пользователя без привилегий root для выполнения таких задач минимизирует риск изменения или удаления важных файлов или случайного выполнения других опасных операций.

Создайте в рабочем пространстве файл с именем ~/kube-cluster/initial.yml:

nano ~/kube-cluster/initial.yml

Добавьте в файл следующую строку сценария play для создания пользователя без привилегий root с привилегиями sudo на всех серверах. Сценарий в Ansible — это набор выполняемых шагов, нацеленных на определенные серверы и группы. Следующий сценарий создаст пользователя без привилегий root с привилегиями sudo:

- hosts: all

  become: yes
  
  tasks:
  
    - name: create the 'ubuntu' user
    
      user: name=ubuntu append=yes state=present createhome=yes shell=/bin/bash
      

    - name: allow 'ubuntu' to have passwordless sudo
    
      lineinfile:
      
        dest: /etc/sudoers
        
        line: 'ubuntu ALL=(ALL) NOPASSWD: ALL'
        
        validate: 'visudo -cf %s'
        

    - name: set up authorized keys for the ubuntu user
    
      authorized_key: user=ubuntu key="{{item}}"
      
      with_file:
      
        - ~/.ssh/id_rsa.pub
        

Далее приведено детальное описание операций, выполняемых этим плейбуком:
•	Создает пользователя без привилегий root с именем ubuntu.
•	Настраивает файл sudoers, чтобы пользователь ubuntu мог запускать команды sudo без ввода пароля.
•	Добавляет на локальный компьютер открытый ключ (обычно ~/.ssh/id_rsa.pub) в список авторизованных ключей удаленного пользователя ubuntu. Это позволит вам подключаться к каждому серверу через SSH под именем пользователя ubuntu.
Сохраните и закройте файл после добавления текста.
Затем запустите плейбук на локальном компьютере:

ansible-playbook -i hosts ~/kube-cluster/initial.yml


Выполнение команды займет от двух до пяти минут. После завершения вы увидите примерно следующий результат:

Output

PLAY [all] ****


TASK [Gathering Facts] ****

ok: [master]

ok: [worker1]

ok: [worker2]


TASK [create the 'ubuntu' user] ****

changed: [master]

changed: [worker1]

changed: [worker2]


TASK [allow 'ubuntu' user to have passwordless sudo] ****

changed: [master]

changed: [worker1]

changed: [worker2]


TASK [set up authorized keys for the ubuntu user] ****

changed: [worker1] => (item=ssh-rsa AAAAB3...)

changed: [worker2] => (item=ssh-rsa AAAAB3...)

changed: [master] => (item=ssh-rsa AAAAB3...)


PLAY RECAP ****

master                     : ok=5    changed=4    unreachable=0    failed=0

worker1                    : ok=5    changed=4    unreachable=0    failed=0

worker2                    : ok=5    changed=4    unreachable=0    failed=0

Теперь предварительная настройка завершена, и вы можете перейти к установке зависимостей Kubernetes.

## Шаг 3 — Установка зависимостей Kubernetetes
В этом разделе вы научитесь устанавливать требующиеся Kubernetes пакеты уровня операционной системы с помощью диспетчера пакетов Ubuntu. Вот эти пакеты:
•	Docker — среда исполнения контейнеров. Это компонент, который запускает ваши контейнеры. В настоящее время для Kubernetes активно разрабатывается поддержка других сред исполнения, в том числе rkt.
•	kubeadm — инструмент командной строки, который устанавливает и настраивает различные компоненты кластера стандартным образом.
•	kubelet — системная служба / программа, которая работает на всех узлах и обрабатывает операции на уровне узлов.
•	kubectl — инструмент командной строки, используемый для отправки команд на кластер через сервер API.
Создайте в рабочем пространстве файл с именем ~/kube-cluster/kube-dependencies.yml:

nano ~/kube-cluster/kube-dependencies.yml

Добавьте в файл следующие сценарии, чтобы установить данные пакеты на ваши серверы:

- hosts: all

  become: yes
  
  tasks:
  
   - name: install Docker
   
     apt:
     
       name: docker.io
       
       state: present
       
       update_cache: true
       

   - name: install APT Transport HTTPS
   
     apt:
     
       name: apt-transport-https
       
       state: present
       

   - name: add Kubernetes apt-key
   
     apt_key:
     
       url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
       
       state: present
       

   - name: add Kubernetes' APT repository
   
     apt_repository:
     
      repo: deb http://apt.kubernetes.io/ kubernetes-xenial main
      
      state: present
      
      filename: 'kubernetes'
      

   - name: install kubelet
   
     apt:
     
       name: kubelet=1.14.0-00
       
       state: present
       
       update_cache: true
       

   - name: install kubeadm
   
     apt:
     
       name: kubeadm=1.14.0-00
       
       state: present
       

- hosts: master

  become: yes
  
  tasks:
  
   - name: install kubectl
   
     apt:
     
       name: kubectl=1.14.0-00
       
       state: present
       
       force: yes
       

Первый сценарий в плейбуке выполняет следующие операции:
•	Устанавливает среду исполнения контейнеров Docker.
•	Устанавливает apt-transport-https, позволяя добавлять внешние источники HTTPS в список источников APT.
•	Добавляет ключ apt-key репозитория Kubernetes APT для проверки ключей.
•	Добавляет репозиторий Kubernetes APT в список источников APT ваших удаленных серверов.
•	Устанавливает kubelet и kubeadm.
Второй сценарий состоит из одной задачи, которая устанавливает kubectl на главном узле.
Сохраните файл и закройте его после завершения.
Затем запустите плейбук на локальном компьютере:

ansible-playbook -i hosts ~/kube-cluster/kube-dependencies.yml

После завершения вы увидите примерно следующий результат:

Output

PLAY [all] ****


TASK [Gathering Facts] ****

ok: [worker1]

ok: [worker2]

ok: [master]


TASK [install Docker] ****

changed: [master]

changed: [worker1]

changed: [worker2]


TASK [install APT Transport HTTPS] *****

ok: [master]

ok: [worker1]

changed: [worker2]


TASK [add Kubernetes apt-key] *****

changed: [master]

changed: [worker1]

changed: [worker2]


TASK [add Kubernetes' APT repository] *****

changed: [master]

changed: [worker1]

changed: [worker2]


TASK [install kubelet] *****

changed: [master]

changed: [worker1]

changed: [worker2]


TASK [install kubeadm] *****

changed: [master]

changed: [worker1]

changed: [worker2]


PLAY [master] *****


TASK [Gathering Facts] *****

ok: [master]


TASK [install kubectl] ******

ok: [master]


PLAY RECAP ****

master                     : ok=9    changed=5    unreachable=0    failed=0

worker1                    : ok=7    changed=5    unreachable=0    failed=0

worker2                    : ok=7    changed=5    unreachable=0    failed=0

После выполнения на всех удаленных серверах будут установлены Docker, kubeadm и kubelet. kubectl не является обязательным компонентом и требуется только для выполнения команд кластера. В этом контексте имеет смысл производить установку только на главный узел, поскольку вы будете запускать команды kubectl только на главном узле. Однако следует отметить, что команды kubectl можно запускать с любых рабочих узлов и на любом компьютере, где их можно установить и настроить для указания на кластер.
Теперь все системные зависимости установлены. Далее мы настроим главный узел и проведем инициализацию кластера.

## Шаг 4 — Настройка главного узла
На этом шаге вы настроите главный узел. Прежде чем создавать любые плейбуки, следует познакомиться с концепциями подов и плагинов сети подов, которые будут использоваться в вашем кластере.
Под — это атомарная единица, запускающая один или несколько контейнеров. Эти контейнеры используют общие ресурсы, такие как файловые тома и сетевые интерфейсы. Под — это базовая единица планирования в Kubernetes: все контейнеры в поде гарантированно запускаются на том же узле, который назначен для этого пода.
Каждый под имеет собственный IP-адрес, и под на одном узле должен иметь доступ к поду на другом узле через IP-адрес пода. Контейнеры в одном узле могут легко взаимодействовать друг с другом через локальный интерфейс. Однако связь между подами более сложная, и для нее требуется отдельный сетевой компонент, обеспечивающий прозрачную маршрутизацию трафика между подами на разных узлах.
Эту функцию обеспечивают плагины сети подов. Для этого кластера мы используем стабильный и производительный плагин Flannel.

Создайте на локальном компьютере плейбук Ansible с именем master.yml:

nano ~/kube-cluster/master.yml

Добавьте в файл следующий сценарий для инициализации кластера и установки Flannel:

- hosts: master

  become: yes
  
  tasks:
  
    - name: initialize the cluster
    
      shell: kubeadm init --pod-network-cidr=10.244.0.0/16 >> cluster_initialized.txt
      
      args:
      
        chdir: $HOME
        
        creates: cluster_initialized.txt
        

    - name: create .kube directory
    
      become: yes
      
      become_user: ubuntu
      
      file:
      
        path: $HOME/.kube
        
        state: directory
        
        mode: 0755
        

    - name: copy admin.conf to user's kube config
    
      copy:
      
        src: /etc/kubernetes/admin.conf
        
        dest: /home/ubuntu/.kube/config
        
        remote_src: yes
        
        owner: ubuntu
        

    - name: install Pod network
    
      become: yes
      
      become_user: ubuntu
      
      shell: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml >> pod_network_setup.txt
      
      args:
      
        chdir: $HOME
        
        creates: pod_network_setup.txt
        

Далее приведена детализация этого сценария:

•	Первая задача инициализирует кластер посредством запуска kubeadm init. При передаче аргумента --pod-network-cidr=10.244.0.0/16 задается частная подсеть, из которой назначаются IP-адреса подов. Flannel использует вышеуказанную подсеть по умолчанию, и мы предпишем kubeadm использовать ту же подсеть.

•	Вторая задача создает каталог .kube по адресу /home/ubuntu. В это каталоге будут храниться данные конфигурации, в том числе файлы ключа администратора, необходимые для подключения к кластеру, и адрес API кластера.

•	Третья задача копирует файл /etc/kubernetes/admin.conf, сгенерированный kubeadm init в домашнем каталоге пользователя без привилегий root. Это позволит вам использовать kubectl для доступа к новому кластеру.

•	Последняя задача запускает kubectl apply для установки Flannel. Синтаксис kubectl apply -f descriptor.[yml|json] предписывает kubectl создать объекты, описанные в файле descriptor.[yml|json]. Файл kube-flannel.yml содержит описания объектов, требуемых для настроки Flannel в кластере.

Сохраните файл и закройте его после завершения.

Запустите плейбук на локальной системе с помощью команды:

ansible-playbook -i hosts ~/kube-cluster/master.yml

После завершения вы увидите примерно следующий результат:

Output


PLAY [master] ****


TASK [Gathering Facts] ****

ok: [master]


TASK [initialize the cluster] ****

changed: [master]


TASK [create .kube directory] ****

changed: [master]


TASK [copy admin.conf to user's kube config] *****

changed: [master]


TASK [install Pod network] *****

changed: [master]


PLAY RECAP ****

master                     : ok=5    changed=4    unreachable=0    failed=0

Чтобы проверить статус главного узла, подключитесь к нему через SSH с помощью следующей команды:

ssh ubuntu@master_ip

Запустите на главном узле следующую команду:

kubectl get nodes

Результат будет выглядеть следующим образом:

Output

NAME      STATUS    ROLES     AGE       VERSION

master    Ready     master    1d        v1.24.0

В результатах показано, что главный узел master завершил выполнение всех задач инициализации и находится в состоянии готовности Ready, из которого он может принимать подключения от рабочих узлов и выполнять задачи, отправленные на сервер API. Теперь вы можете добавить рабочие узлы с локального компьютера.

## Шаг 5 — Настройка рабочих узлов
Для добавления рабочих узлов в кластер нужно запустить на каждом из них отдельную команду. Эта команда предоставляет всю необходимую информацию о кластере, включая IP-адрес, порт сервера API главного узла и защищенный токен. К кластеру могут подключаться только те узлы, которые проходят проверку с защищенным токеном.

Вернитесь в рабочее пространство и создайте плейбук с именем workers.yml:

nano ~/kube-cluster/workers.yml

Добавьте в файл следующий текст для добавления рабочих узлов в кластер:

- hosts: master

  become: yes
  
  gather_facts: false
  
  tasks:
  
    - name: get join command
    
      shell: kubeadm token create --print-join-command
      
      register: join_command_raw
      

    - name: set join command
    
      set_fact:
      
        join_command: "{{ join_command_raw.stdout_lines[0] }}"
        


- hosts: workers

  become: yes
  
  tasks:
  
    - name: join cluster
    
      shell: "{{ hostvars['master'].join_command }} >> node_joined.txt"
      
      args:
      
        chdir: $HOME
        
        creates: node_joined.txt
        

Вот что делает этот плейбук:

•	Первый сценарий получает команду join, которую нужно запустить на рабочих узлах. Эта команда имеет следующий форматt: kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>. После получения фактической команды с правильными значениями token и hash задача задает их как фактические, чтобы следующий сценарий имел доступ к этой информации.
  
•	Второй сценарий содержит одну задачу, которая запускает команду join на всех рабочих узлах. После завершения этой задачи два рабочих узла становятся частью кластера.
Сохраните файл и закройте его после завершения.
  
Запустите плейбук на локальном компьютере:
  
ansible-playbook -i hosts ~/kube-cluster/workers.yml
  
После завершения вы увидите примерно следующий результат:
  
Output
  
PLAY [master] ****
  

TASK [get join command] ****
  
changed: [master]
  

TASK [set join command] *****
  
ok: [master]
  

PLAY [workers] *****
  

TASK [Gathering Facts] *****
  
ok: [worker1]
  
ok: [worker2]
  

TASK [join cluster] *****
  
changed: [worker1]
  
changed: [worker2]
  

PLAY RECAP *****
  
master                     : ok=2    changed=1    unreachable=0    failed=0
  
worker1                    : ok=2    changed=1    unreachable=0    failed=0
  
worker2                    : ok=2    changed=1    unreachable=0    failed=0
  
Теперь рабочие узлы добавлены, ваш кластер полностью настроен и готов к работе, а рабочие узлы готовы к выполнению рабочих задач. Перед назначением приложений следует убедиться, что кластер работает надлежащим образом.
  
## Шаг 6 — Проверка кластера
Иногда при установке и настройке кластера может произойти ошибка, если один из узлов отключен или имеются проблемы сетевого соединения между главным узлом и рабочими узлами. Сейчас мы проверим кластер и убедимся, что все узлы работают правильно.
  
Вам нужно будет проверить текущее состояние кластера с главного узла, чтобы убедиться в готовности всех узлов. Если вы отключились от главного узла, вы можете снова подключиться к нему через SSH с помощью следующей команды:
  
ssh ubuntu@master_ip
  
Затем выполните следующую команду, чтобы получить данные о статусе кластера:
  
kubectl get nodes
  
Вы увидите примерно следующий результат:
  
Output
  
NAME      STATUS    ROLES     AGE       VERSION
  
master    Ready     master    1d        v1.24.0
  
worker1   Ready     <none>    1d        v1.24.0
  
worker2   Ready     <none>    1d        v1.24.0
  
Если на всех ваших узлах отображается значение Ready для параметра STATUS, это означает, что они являются частью кластера и готовы к выполнению рабочих задач.
Если же на некоторых узлах отображается значение NotReady для параметра STATUS, это может означать, что настройка рабочих узлов еще не завершена. Подождите от пяти до десяти минут, а затем снова запустите команду kubectl get nodes и проверьте полученные результаты. Если для некоторых узлов еще отображается статус NotReady, вам нужно проверить и заново запустить команды, которые выполнялись на предыдущих шагах.
Теперь кластер успешно проверен, и мы запланируем запуск на кластере образца приложения Nginx.
  
## Шаг 7 — Запуск приложения на кластере
Теперь вы можете развернуть на кластере любое приложение в контейнере. Для удобства мы развернем Nginx с помощью Deployments и Services и посмотрим, как можно развернуть это приложение на кластере. Вы можете использовать приведенные ниже команды для других приложений в контейнерах, если вы измените имя образа Docker и дополнительные параметры (такие как ports и volumes).
  
Запустите на главном узле следующую команду для создания развертывания с именем nginx:
  
kubectl create deployment nginx --image=nginx
  
Развертывание — это тип объекта Kubernetes, обеспечивающий постоянную работу определенного количества подов на основе заданного шаблона, даже в случае неисправности пода в течение срока службы кластера. Вышеуказанное развертывание создаст под с одним контейнером из образа Nginx Docker в реестре Docker.
  
Запустите следующую команду, чтобы создать службу nginx, которая сделает приложение общедоступным. Для этого используется схема NodePort, которая делает под доступным на произвольном порту, который открывается на каждом узле кластера:
  
kubectl expose deploy nginx --port 80 --target-port 80 --type NodePort
  
Службы — это еще один тип объектов Kubernetes, который открывает внутренние службы кластера для внутренних и внешних клиентов. Они поддерживают запросы балансировки нагрузки на разные поды и являются неотъемлемым компонентом Kubernetes, который часто взаимодействует с другими компонентами.
  
Запустите следующую команду:
  
kubectl get services
  
Будет выведен текст следующего вида:
  
Output
  
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP           PORT(S)             AGE
  
kubernetes   ClusterIP   10.96.0.1        <none>                443/TCP             1d
  
nginx        NodePort    10.109.228.209   <none>                80:nginx_port/TCP   40m
  
В третьей сроке результатов указан номер порта, на котором запущен Nginx. Kubernetes автоматически назначает случайный порт с номером выше 30000 и при этом проверяет, не занят ли этот порт другой службой.
Чтобы убедиться в работе всех элементов, откройте адрес http://worker_1_ip:nginx_port или http://worker_2_ip:nginx_port в браузере на локальном компьютере. Вы увидите знакомую начальную страницу Nginx.
  
Если вы захотите удалить приложение Nginx, предварительно удалите службу nginx с главного узла:
  
kubectl delete service nginx
  
Запустите следующую команду, чтобы проверить удаление службы:
  
kubectl get services
  
Результат будет выглядеть следующим образом:
  
Output
  
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP           PORT(S)        AGE
  
kubernetes   ClusterIP   10.96.0.1        <none>                443/TCP        1d
  
Затем удалите развертывание:
  
kubectl delete deployment nginx
  
Запустите следующую команду для проверки успешности выполнения:
  
kubectl get deployments
  

Output
  
No resources found.
  
  
## Заключение
В этом обучающем модуле вы научились успешно настраивать кластер Kubernetes в Ubuntu 18.04, используя для автоматизации Kubeadm и Ansible.
Если вы не знаете, что дальше делать с настроенным кластером, попробуйте развернуть на нем собственные приложения и службы. Далее приведен список ссылок с дополнительной информацией, которая будет вам полезна:
  
•	Докеризация приложений — детальные примеры контейнеризации приложений с помощью Docker.
  
•	Обзор подов — детальное описание принципов работы подов и их отношений с другими объектами Kubernetes. Поды используются в Kubernetes повсеместно, так что понимание этой концепции упростит вашу работу.
  
•	Обзор развертывания — обзор концепции развертывания. С его помощью проще понять принципы работы таких контроллеров как развертывания, поскольку они часто используются для масштабирования в приложениях без сохранения состояния и для автоматического восстановления поврежденных приложений.
  
•	Обзор служб — рассказывает о службах, еще одном часто используемом объекте кластеров Kubernetes. Понимание типов служб и доступных для них опций важно для использования приложений с сохранением состояния и без сохранения состояния.
  
Другие важные концепции, полезные при развертывании рабочих приложений: тома, входы и секреты.
В Kubernetes имеется множество функций и возможностей. Официальная документация Kubernetes — лучший источник информации о концепциях, руководств по конкретным задачам и ссылок API для различных объектов.

  
# Создайте свою первую диаграмму Helm на платформе Базис. Digital Energy


## Знакомство
 Итак, у вас есть кластер Kubernetes, развернутый на технологической платформе ПАО Ростелеком Базис. Digital Energy, запущенный и настроенный Helm v3.x, но как вы запускаете свои приложения на нем? Это руководство проведет вас через процесс создания вашей первой диаграммы, объяснив, что входит в эти пакеты и инструменты, которые вы используете для их разработки. К концу вы должны понять преимущества использования Helm для доставки собственных приложений в кластер.
Для типичного облачного приложения с 3-уровневой архитектурой на приведенной ниже схеме показано, как его можно описать в терминах объектов Kubernetes. В этом примере каждый уровень состоит из объекта Deployment и Service и может дополнительно определять объекты ConfigMap или Secret. Каждый из этих объектов обычно определяется в отдельных файлах YAML и передается в программу командной строки kubectl.
  
  ![Screenshot_1](https://github.com/DanilkaCrazy/Helm-create-clasters-Kubernetes/assets/95550202/90723735-dc11-4666-878b-c41f30fdb8cb)

 
Диаграмма Helm инкапсулирует каждое из этих определений YAML, предоставляет механизм настройки во время развертывания и позволяет определять метаданные и документацию, которые могут быть полезны при совместном использовании пакета. Helm может быть полезен в различных сценариях:
  
•	Поиск и использование популярного программного обеспечения, упакованного в виде диаграмм Kubernetes
  
•	Совместное использование собственных приложений в виде диаграмм Kubernetes
  
•	Создание воспроизводимых сборок приложений Kubernetes
  
•	Интеллектуальное управление определениями объектов Kubernetes
  
•	Управление выпусками пакетов Helm
  
Давайте рассмотрим второй и третий сценарии, создав нашу первую диаграмму.
  
## Шаг 1: Создайте свою первую диаграмму
Лучший способ начать работу с новой диаграммой — использовать команду helm create для создания примера, на котором мы можем основываться. 
  
Используйте эту команду для создания новой диаграммы с именем mychart в новом каталоге:
  
helm create mychart
  
Helm создаст новый каталог в вашем проекте под названием mychart со структурой, показанной ниже. Давайте перейдем к нашему новому Chart, чтобы узнать, как это работает.
  
mychart
  
|-- Chart.yaml
  
|-- charts
  
|-- templates
  
|   |-- NOTES.txt
  
|   |-- _helpers.tpl
  
|   |-- deployment.yaml
  
|   |-- ingress.yaml
  
|   `-- service.yaml
  
`-- values.yaml
  
## Шаблоны
Самая важная часть головоломки – это каталог templates/ . Здесь Helm находит определения YAML для ваших служб, развертываний и других объектов Kubernetes. Если у вас уже есть определения для приложения, все, что вам нужно сделать, это заменить созданные файлы YAML на свои собственные. В итоге получается рабочая диаграмма, которую можно развернуть с помощью команды helm install.
  
Однако стоит отметить, что каталог называется templates, и Helm запускает каждый файл в этом каталоге через движок рендеринга шаблонов Go. Helm расширяет язык шаблонов, добавляя ряд утилитарных функций для написания Chart’s. Откройте файл service.yaml, чтобы узнать, как это выглядит:
  
apiVersion: v1
  
kind: Service
  
metadata:
  
name: {{ template "fullname" . }}
  
labels:
  
    chart: "{{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}"
  
spec:
  
type: {{ .Values.service.type }}
  
ports:
  
- port: {{ .Values.service.externalPort }}
  
    targetPort: {{ .Values.service.internalPort }}
  
    protocol: TCP
  
    name: {{ .Values.service.name }}
  
selector:
  
    app: {{ template "fullname" . }}
  
  
Это базовое определение службы с использованием шаблонов. При развертывании диаграммы Helm сгенерирует определение, которое будет больше похоже на допустимую службу. Мы можем выполнить  helm install и включить отладку для проверки сгенерированных определений:
  
helm install example --dry-run --debug ./mychart
  
...
  
# Source: mychart/templates/service.yaml
  
apiVersion: v1
  
kind: Service
  
metadata:
  
name: pouring-puma-mychart
  
labels:
  
    chart: "mychart-0.1.0"
  
spec:
  
type: ClusterIP
  
ports:
  
- port: 80
  
    targetPort: 80
  
    protocol: TCP
  
    name: nginx
  
selector:
  
    app: pouring-puma-mychart
  
...
  
## Valurs
Шаблон в service.yaml использует объекты, специфичные для Helm .Chart и .Values. Первый предоставляет метаданные о диаграмме для ваших определений, таких как имя или версия. Последнее объект .Values является ключевым элементом диаграмм Helm, используемым для предоставления конфигурации, которая может быть задана во время развертывания. Значения по умолчанию для этого объекта определены в файле values.yaml. Попробуйте изменить значение по умолчанию для service.internalPort и выполнить еще один сухой запуск, вы обнаружите, что targetPort в службе и containerPort в развертывании изменяются. Значение service.internalPort используется здесь для обеспечения правильной совместной работы объектов Service и Deployment. Использование шаблонов может значительно уменьшить шаблон и упростить ваши определения.
  
Если пользователь диаграммы хочет изменить конфигурацию по умолчанию, он может предоставить переопределения непосредственно в командной строке:
  
helm install --dry-run --debug ./mychart --set service.internalPort=8080
  
Для более сложной настройки пользователь может указать файл YAML, содержащий переопределения с помощью --values парамета.
  
## Помощники и другие функции  
Шаблон service.yaml также использует частичные значения, определенные в _helpers.tpl, а также такие функции, как replace. Документация Helm содержит более глубокое пошаговое руководство по языку шаблонов, объясняющее, как функции, частичные части и управление потоком могут использоваться при разработке диаграммы.
  
## Документация
Другим полезным файлом в каталоге templates/ является файл NOTES.txt. Это шаблонный текстовый файл, который распечатывается после успешного развертывания диаграммы. Как мы увидим, когда мы развернем нашу первую диаграмму, это полезное место для краткого описания следующих шагов по использованию диаграммы. Поскольку NOTES.txt выполняется через модуль шаблонов, вы можете использовать шаблоны для распечатки рабочих команд для получения IP-адреса или получения пароля от объекта Secret.
  
## Метаданные
Как упоминалось ранее, диаграмма Helm состоит из метаданных, которые используются для описания того, что представляет собой приложение, определения ограничений на минимально необходимые Kubernetes и/или версии Helm и управления версией диаграммы. Все эти метаданные находятся в файле Chart.yaml. Документация Helm описывает различные поля для этого файла.
  
## Шаг 2. Развертывание первой диаграммы
Диаграмма, созданная на предыдущем шаге, настроена на запуск сервера NGINX, предоставляемого через службу Kubernetes. По умолчанию диаграмма создает службу типа ClusterIP, поэтому NGINX будет предоставляться только внутри кластера. Чтобы получить к нему внешний доступ, мы будем использовать тип NodePort. Мы также можем установить название выпуска Helm, чтобы мы могли легко вернуться к нему. Давайте продолжим и развернем нашу диаграмму NGINX с помощью команды helm install:
  

helm install example ./mychart --set service.type=NodePort
  
NAME:   example
  
LAST DEPLOYED: Tue May  2 20:03:27 2017
  
NAMESPACE: default
  
STATUS: DEPLOYED
  

RESOURCES:
  
==> v1/Service
  
NAME             CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
  
example-mychart  10.0.0.24   <nodes>      80:30630/TCP  0s
  

==> v1beta1/Deployment
  
NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
  
example-mychart  1        1        1           0          0s
  


NOTES:
  
1. Get the application URL by running these commands:
  
export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services example-mychart)
  
export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  
echo http://$NODE_IP:$NODE_PORT/
  
Выходные данные helm install отображают удобную сводку состояния выпуска, какие объекты были созданы, и визуализированный файл NOTES.txt чтобы объяснить, что делать дальше. Выполните команды в выходных данных, чтобы получить URL-адрес для доступа к службе NGINX и получить его в браузере.
  
![Screenshot_2](https://github.com/DanilkaCrazy/Helm-create-clasters-Kubernetes/assets/95550202/731568ab-668b-4ac4-aa61-d9deff1ed82c)

  
Если все прошло хорошо, вы должны увидеть страницу приветствия NGINX, как показано выше. Поздравляю! Вы только что развернули свою самую первую услугу, упакованную в виде диаграммы Helm!
  
## Шаг 3. Изменение диаграммы для развертывания пользовательской службы
Созданная диаграмма создает объект Deployment, предназначенный для запуска образа, предоставляемого значениями по умолчанию. Это означает, что все, что нам нужно сделать для запуска другой службы, это изменить изображение, на которое указывает ссылка в values.yaml.
  
Мы собираемся обновить диаграмму, чтобы запустить приложение списка дел, доступное в Docker Hub. В файле values.yaml обновите ключи изображения, чтобы они ссылались на изображение списка дел:
  
image:
  
repository: prydonius/todo
  
tag: 1.0.0
  
pullPolicy: IfNotPresent
  
При разработке диаграммы рекомендуется запускать ее через линтер, чтобы убедиться, что вы следуете рекомендациям и что ваши шаблоны хорошо сформированы. Выполните команду helm lint, чтобы увидеть линтер в действии:
  
helm lint ./mychart
  
==> Linting ./mychart
  
[INFO] Chart.yaml: icon is recommended
  

1 chart(s) linted, no failures
  
Линтер не обнаружил какие-либо серьезные проблемы с Chart, так что мы готовы к работе. Однако, в качестве примера, вот что может вывести линтер, если вам удалось что-то сделать не так:
  
echo "malformed" > mychart/values.yaml
  
helm lint ./mychart
  
==> Linting mychart
  
[INFO] Chart.yaml: icon is recommended
  
[ERROR] values.yaml: unable to parse YAML
  
    error converting YAML to JSON: yaml: line 34: could not find expected ':'
  

Error: 1 chart(s) linted, 1 chart(s) failed
  
На этот раз линтер сообщает нам, что не удалось правильно проанализировать мой файл values.yaml. С подсказкой номера строки мы можем легко найти исправление ошибки, которую мы ввели.
  
Теперь, когда Chart снова действительна, запустите helm install еще раз, чтобы развернуть приложение списка дел:
  
helm install example2 ./mychart --set service.type=NodePort
  
NAME:   example2
  
LAST DEPLOYED: Wed May  3 12:10:03 2017
  
NAMESPACE: default
  
STATUS: DEPLOYED
  

RESOURCES:
  
==> v1/Service
  
NAME              CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
  
example2-mychart  10.0.0.78   <nodes>      80:31381/TCP  0s
  

==> apps/v1/Deployment
  
NAME              DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
  
example2-mychart  1        1        1           0          0s
  


NOTES:
  
1. Get the application URL by running these commands:
  
export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services example2-mychart)
  
export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  
echo http://$NODE_IP:$NODE_PORT/
  
Еще раз, мы можем выполнить команды в NOTES, чтобы получить URL-адрес для доступа к нашему приложению.
  
 ![Screenshot_3](https://github.com/DanilkaCrazy/Helm-create-clasters-Kubernetes/assets/95550202/29ca0719-8d06-4ec4-92f7-af7fc998056d)

  
Если контейнеры для приложений уже созданы, их можно запустить вместе с chart, обновив values по умолчанию или шаблон Deployment.
  
## Шаг 4: Упакуйте все это, чтобы поделиться
До сих пор в этом учебнике мы использовали команду helm install для установки локальной распакованной диаграммы. Однако, если вы хотите поделиться своими диаграммами со своей командой или сообществом, ваши потребители обычно устанавливают диаграммы из пакета tar. Мы можем использовать helm package для создания пакета tar:
  
helm package ./mychart
  
Helm создаст пакет mychart-0.1.0.tgz в нашем рабочем каталоге, используя имя и версию из метаданных, определенных в файле Chart.yaml. Пользователь может выполнить установку из этого пакета вместо локального каталога, передав пакет в качестве параметра для helm install.
  
helm install example3 mychart-0.1.0.tgz --set service.type=NodePort
  
## Репозитории
Чтобы упростить обмен пакетами, Helm имеет встроенную поддержку установки пакетов с HTTP-сервера. Helm считывает индекс репозитория, размещенный на сервере, который описывает, какие пакеты диаграмм доступны и где они расположены.
  
Мы можем использовать команду helm serve для запуска локального репозитория для обслуживания нашего chart.
  
helm serve
  
Regenerating index. This may take a moment.
  
Now serving you on 127.0.0.1:8879
  
Теперь в отдельном окне терминала вы сможете увидеть свой Chart в локальном репозитории и установить его оттуда:
  
helm search local
  
NAME            VERSION DESCRIPTION
  
local/mychart   0.1.0   A Helm chart for Kubernetes
  

helm install example4 local/mychart --set service.type=NodePort
  
Чтобы настроить удаленный репозиторий, вы можете следовать руководству в документации Helm.
  
## Зависимости
По мере того, как приложения, упакованные в виде диаграмм, становятся все более сложными, может оказаться, что вам нужно извлечь зависимость, такую как база данных. Helm позволяет указать sub-charts, которые будут созданы в рамках того же релиза. Чтобы определить зависимость, создайте файл requirements.yaml в корневом каталоге диаграммы:
  
cat > ./mychart/requirements.yaml <<EOF
  
dependencies:
  
- name: mariadb
  
version: 0.6.0
  
repository: https://charts.helm.sh/stable
  
EOF
  
Подобно файлу зависимостей языка среды выполнения (например, требованиям Python.txt), файл requirements.yaml позволяет управлять зависимостями Chart’s и их версиями. При обновлении зависимостей создается файл блокировки, чтобы последующая выборка зависимостей использовала известную рабочую версию. Выполните следующую команду, чтобы получить определенную нами зависимость MariaDB:
  
helm dep update ./mychart
  
Hang tight while we grab the latest from your chart repositories...
  
...Unable to get an update from the "local" chart repository (http://127.0.0.1:8879/charts):
  
    Get http://127.0.0.1:8879/charts/index.yaml: dial tcp 127.0.0.1:8879: getsockopt: connection refused
  
...Successfully got an update from the "bitnami" chart repository
  
...Successfully got an update from the "incubator" chart repository
  
Update Complete. *Happy Helming!*
  
Saving 1 charts
  
Downloading mariadb from repo
  
$ ls ./mychart/charts
  
mariadb-0.6.0.tgz
  
Helm нашел подходящую версию в репозитории bitnami и извлек ее в каталог sub-chart моего chart. Теперь, когда мы установим chart, мы увидим, что объекты MariaDB также созданы:
  
helm install example5 ./mychart --set service.type=NodePort
  
NAME:   example5
  
LAST DEPLOYED: Wed May  3 16:28:18 2017
  
NAMESPACE: default
  
STATUS: DEPLOYED
  

RESOURCES:
  
==> v1/Secret
  
NAME              TYPE    DATA  AGE
  
example5-mariadb  Opaque  2     1s
  

==> v1/ConfigMap
  
NAME              DATA  AGE
  
example5-mariadb  1     1s
  

==> v1/PersistentVolumeClaim
  
NAME              STATUS  VOLUME                                    CAPACITY  ACCESSMODES  AGE
  
example5-mariadb  Bound   pvc-229f9ed6-3015-11e7-945a-66fc987ccf32  8Gi       RWO          1s
  

==> v1/Service
  
NAME              CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
  
example5-mychart  10.0.0.144  <nodes>      80:30896/TCP  1s
  
example5-mariadb  10.0.0.108  <none>       3306/TCP      1s
  

==> apps/v1/Deployment
  
NAME              DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
  
example5-mariadb  1        1        1           0          1s
  
example5-mychart  1        1        1           0          1s
  


NOTES:
  
1. Get the application URL by running these commands:
  
export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services example5-mychart)
  
export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  
echo http://$NODE_IP:$NODE_PORT/
  


